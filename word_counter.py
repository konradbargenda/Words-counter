# -*- coding: utf-8 -*-
"""Word counter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ML4CqXlAYCEYgKy4Wh-orkAh2osO51vY
"""

from multiprocessing import cpu_count

cpu_count()

# Commented out IPython magic to ensure Python compatibility.
# %%file pmap.py
# from concurrent.futures import ProcessPoolExecutor
# from time import sleep, time
# 
# def f(x):
#     sleep(1)
#     return x*x
# 
# L = list(range(8))
# 
# if __name__ == '__main__':
# 
#     begin = time()
#     with ProcessPoolExecutor() as pool:
# 
#         result = sum(pool.map(f, L))
#     end = time()
# 
#     print(f"result = {result} and time = {end-begin}")
#

import sys
!{sys.executable} pmap.py

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from concurrent.futures import ThreadPoolExecutor
# 
# with ThreadPoolExecutor() as pool:
# 
#     results = sum(pool.map(f, L))
# 
# print(results)

# Commented out IPython magic to ensure Python compatibility.
# %mkdir books

from glob import glob
from collections import defaultdict
from operator import itemgetter
from itertools import chain
from concurrent.futures import ThreadPoolExecutor

def mapper(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        data = f.read()
    data = data.strip().replace(".", "").replace(",", "").lower().split()
    return [(w, 1) for w in data]

def partitioner(mapped_values):
    res = defaultdict(list)
    for w, c in mapped_values:
        res[w].append(c)
    return res.items()

def reducer(item):
    w, v = item
    return (w, sum(v))

def map_reduce(filenames):
    with ThreadPoolExecutor() as executor:
        # Map step
        mapped_values = list(chain.from_iterable(executor.map(mapper, filenames)))

        # Partition step
        partitioned_data = partitioner(mapped_values)

        # Reduce step
        reduced_values = list(executor.map(reducer, partitioned_data))

    return sorted(reduced_values, key=itemgetter(1), reverse=True)

if __name__ == "__main__":
    # List of text files to process (ensuring exactly 5 files)
    filenames = glob("hugo*.txt")[:5]

    # Perform Map-Reduce and get word counts
    word_counts = map_reduce(filenames)

    # Print the word counts
    for word, count in word_counts:
        print(f"{word}: {count}")